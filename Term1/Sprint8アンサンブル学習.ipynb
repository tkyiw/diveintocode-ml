{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pd.read_csv('../kaggledata/train.csv')\n",
    "\n",
    "X = train[['GrLivArea', 'YearBuilt']]\n",
    "y = train[['SalePrice']]\n",
    "\n",
    "# 対数変換\n",
    "X = np.log(X).values\n",
    "y = np.log(y).values\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "sc.fit_transform(X)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・ベースラインモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "SVM：0.04395091582650826\n",
      "決定木：0.042680977503983995\n",
      "線形回帰：0.04590550537802906\n",
      "XBG：0.03358922969505076\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svr = SVR().fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "svr_mse = mean_squared_error(y_test, svr_pred)\n",
    "\n",
    "# 決定木\n",
    "dt = DecisionTreeRegressor(max_depth=5).fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "dt_mse = mean_squared_error(y_test, dt_pred)\n",
    "\n",
    "# 線形回帰\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test).ravel()\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "\n",
    "# xgboost（参考）\n",
    "xg = xgb.XGBRegressor().fit(X_train, y_train)\n",
    "xg_pred = xg.predict(X_test).ravel()\n",
    "xg_mse = mean_squared_error(y_test, xg_pred)\n",
    "\n",
    "# 評価\n",
    "print(f'SVM：{svr_mse}')\n",
    "print(f'決定木：{dt_mse}')\n",
    "print(f'線形回帰：{lr_mse}')\n",
    "print(f'XBG：{xg_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ① SVM + 決定木 + 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03966782950842762"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern1 = (svr_pred + dt_pred + lr_pred) / 3\n",
    "pattern1 = mean_squared_error(y_test, pattern1)\n",
    "pattern1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ② SVM + 決定木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03881215108581243"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern2 = (svr_pred + dt_pred) / 2\n",
    "pattern2 = mean_squared_error(y_test, pattern2)\n",
    "pattern2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ③ 決定木 + 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0412580005432294"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern3 = (dt_pred + lr_pred) / 2\n",
    "pattern3 = mean_squared_error(y_test, pattern3)\n",
    "pattern3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ④ SVM : 決定木 = 2 : 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038865309076737305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern4 = (svr_pred*2 + dt_pred*3) / 5\n",
    "pattern4 = mean_squared_error(y_test, pattern4)\n",
    "pattern4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">各モデルの推定値を平均するだけでベースラインより良いスコアがでた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木でバギングを検証する。\n",
    "### ベースラインスコア 決定木：0.042680977503983974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(num, sample_num):\n",
    "    '''\n",
    "    バギングを行う関数\n",
    "    '''\n",
    "    num = num #学習回数\n",
    "    sample_num = sample_num #サンプル数\n",
    "    pred = 0 #推定の初期値\n",
    "\n",
    "    for _ in range(num):\n",
    "        # ランダムにsample_num個抽出（重複あり）\n",
    "        idx = np.random.choice(len(X_train), sample_num)\n",
    "\n",
    "        # 学習・推定\n",
    "        model = DecisionTreeRegressor(max_depth=5).fit(X_train[idx], y_train[idx])\n",
    "        pred += model.predict(X_test) / num\n",
    "    \n",
    "    # 評価\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    \n",
    "    return mse, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test ：[12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\n",
      "bagging：[12.2414099  11.92949516 11.70797822 12.22877304 11.77793144]\n",
      "MSE    ：0.036561018097023855\n"
     ]
    }
   ],
   "source": [
    "# 700サンプルで学習を30回行った推定値の平均値に対するMSE\n",
    "mse, pred = bagging(30, 700)\n",
    "print(f'y_test ：{y_test[:5].ravel()}')\n",
    "print(f'bagging：{pred[:5]}')\n",
    "print(f'MSE    ：{mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・本家ランダムフォレストでは・・・"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035758140977409314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdt = RandomForestRegressor(max_depth=5).fit(X_train, y_train)\n",
    "rdt_pred = rdt.predict(X_test)\n",
    "rdt_mse = mean_squared_error(y_test, rdt_pred)\n",
    "rdt_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ベースラインスコアを上回るスコアが得られ、決定木のバギングモデルであるランダムフォレストと同等のスコアであった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_data(X, y, test):\n",
    "    '''\n",
    "    メタモデル用の学習データと検証データを取得する関数\n",
    "    '''\n",
    "    meta_train = np.zeros((len(X), 3)) #学習データの空箱\n",
    "    meta_test = np.zeros((len(test), 3)) #検証データの空箱\n",
    "    n_splits = 6 #分割数\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 学習\n",
    "        model1 = SVR().fit(X_train , y_train)\n",
    "        model2 = DecisionTreeRegressor(max_depth=5).fit(X_train , y_train)\n",
    "        model3 = LinearRegression().fit(X_train , y_train)\n",
    "\n",
    "        # 学習データを推定\n",
    "        pred_train_model1 = model1.predict(X_test).reshape(-1, 1)\n",
    "        pred_train_model2 = model2.predict(X_test).reshape(-1, 1)\n",
    "        pred_train_model3 = model3.predict(X_test).reshape(-1, 1)\n",
    "        \n",
    "        # 検証データを推定\n",
    "        pred_test_model1 = model1.predict(test).reshape(-1, 1)\n",
    "        pred_test_model2 = model2.predict(test).reshape(-1, 1)\n",
    "        pred_test_model3 = model3.predict(test).reshape(-1, 1)\n",
    "        \n",
    "        # メタモデル学習用データを保管\n",
    "        meta_train[test_index, :1] = pred_train_model1\n",
    "        meta_train[test_index, 1:2] = pred_train_model2\n",
    "        meta_train[test_index, 2:] = pred_train_model3\n",
    "        \n",
    "        # メタモデル検証用データを保管\n",
    "        meta_test[:, :1] += pred_test_model1 / n_splits\n",
    "        meta_test[:, 1:2] += pred_test_model2 / n_splits\n",
    "        meta_test[:, 2:] += pred_test_model3 / n_splits\n",
    "    \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ステージ0：0.033658402413554854\n"
     ]
    }
   ],
   "source": [
    "# メタモデル用の学習・検証データ取得\n",
    "meta_train, meta_test = stacking_data(X_train, y_train, X_test)\n",
    "\n",
    "# 学習\n",
    "meta_model = SVR().fit(meta_train, y_train)\n",
    "\n",
    "# 推定\n",
    "pred_meta = meta_model.predict(meta_test)\n",
    "\n",
    "# 評価\n",
    "print(f'ステージ0：{mean_squared_error(y_test, pred_meta)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ここまでの検証の中で最高スコアであった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ステージ1：0.035145494896062335\n",
      "ステージ2：0.03505129747264169\n",
      "ステージ3：0.03686269821979252\n",
      "ステージ4：0.03750921839855495\n",
      "ステージ5：0.037145348301597145\n",
      "ステージ6：0.039482202011180155\n",
      "ステージ7：0.04767650589529238\n",
      "ステージ8：0.03810771911977699\n",
      "ステージ9：0.04312807196575073\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    # メタモデル用の学習・検証データ取得\n",
    "    meta_train, meta_test = stacking_data(meta_train, y_train, meta_test)\n",
    "\n",
    "    # 学習\n",
    "    meta_model = SVR().fit(meta_train, y_train)\n",
    "\n",
    "    # 推定\n",
    "    pred_meta = meta_model.predict(meta_test)\n",
    "\n",
    "    # 評価\n",
    "    print(f'ステージ{i+1}：{mean_squared_error(y_test, pred_meta)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ステージ数を増やすと精度が悪くなる傾向であった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
