{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.このSprintについて\n",
    "\n",
    "Sprintの目的\n",
    "- スクラッチを通してロジスティック回帰を理解する\n",
    "- 分類問題についての基礎を学ぶ\n",
    "\n",
    "どのように学ぶか\n",
    "- スクラッチでロジスティック回帰を実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n",
    "\n",
    "ロジスティック回帰の仮定関数は、線形回帰の仮定関数を シグモイド関数 に通したものです。シグモイド関数は以下の式で表されます。\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{−z}}.\n",
    "$$\n",
    "\n",
    "線形回帰の仮定関数は次の式でした。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$\n",
    "\n",
    "まとめて書くと、ロジスティック回帰の仮定関数は次のようになります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{−\\theta^T \\cdot x}}.\n",
    "$$\n",
    "$x$ : 特徴量ベクトル  \n",
    "$\\theta$ : パラメータ（重み）ベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\\\<br/>\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)}  ,j = 0\\\\<br/>\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\biggl(\\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)} \\biggr) + \\frac{λ}{m}\\theta_j　 ,j\\geq 1\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率  \n",
    "$i$ : サンプルのインデックス  \n",
    "$j$ : 特徴量のインデックス  \n",
    "$m$ : 入力されるデータの数  \n",
    "$h_\\theta()$ : 仮定関数  \n",
    "$x$ : 特徴量ベクトル  \n",
    "$\\theta$ : パラメータ（重み）ベクトル  \n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル  \n",
    "$y^{(i)}$ : i番目のサンプルの正解ラベル  \n",
    "$\\theta_j$ : j番目のパラメータ（重み）  \n",
    "$λ$ : 正則化パラメータ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "\n",
    "仮定関数 $h_\\theta(x)$ の出力がpredict_probaの返り値、さらにその値に閾値を設けて1と0のラベルとしたものがpredictの返り値となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】目的関数\n",
    "以下の数式で表されるロジスティック回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。なお、この数式には正則化項が含まれています。\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m}[−y^{(i)} log(h_θ(x^{(i)})) − (1−y^{(i)}) log(1−h_θ(x^{(i)}))] +\n",
    "\\frac{λ}{2m}\\sum_{j=1}^n\n",
    "θ^2_j\n",
    "$$\n",
    "\n",
    "$m$ : 入力されるデータの数  \n",
    "$h_\\theta()$ : 仮定関数  \n",
    "$x$ : 特徴量ベクトル  \n",
    "$\\theta$ : パラメータ（重み）ベクトル  \n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル  \n",
    "$y^{(i)}$ : i番目のサンプルの正解ラベル  \n",
    "$\\theta_j$ : j番目のパラメータ（重み）  \n",
    "$n$ : 特徴量の数  \n",
    "$λ$ : 正則化パラメータ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, C, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # バイアスの有無\n",
    "        if not self.no_bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            if X_val is not None:\n",
    "                X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "                \n",
    "        # 係数の初期値を標準正規分布に従った乱数で生成\n",
    "        self.coef_ = np.random.normal(loc=0, # 平均\n",
    "                                      scale=0.01, # 標準偏差\n",
    "                                      size=X.shape[1]).reshape(-1, 1)\n",
    "        \n",
    "        # イテレーション数回行う処理\n",
    "        for i in range(self.iter):\n",
    "            # 係数の更新\n",
    "            self.coef_ = self._gradient_descent(X, y)\n",
    "            # 目的関数\n",
    "            self.loss[i] = self._logloss(self._sigmoid_function(X), y)      \n",
    "            if X_val is not None:\n",
    "                self.val_loss[i] = self._logloss(self._sigmoid_function(X_val), y_val)\n",
    "            #verboseをTrueにした際は学習過程を出力    \n",
    "            if self.verbose:\n",
    "                print(f'train_loss：{self.loss[i]}')\n",
    "                if X_val is not None:\n",
    "                    print(f'val_loss：{self.val_loss[i]}')\n",
    "\n",
    "    def _sigmoid_function(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-1 * X @ self.coef_))\n",
    "\n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        最急降下法により学習させる\n",
    "        \"\"\"\n",
    "        _theta = self.coef_[0] - np.mean(self._sigmoid_function(X) - y)\n",
    "        self.coef_ -= self.lr * ((1 / len(X)) * X.T @ (self._sigmoid_function(X) - y) + (1 / (self.C *len(X))) * self.coef_)\n",
    "        if not self.no_bias:\n",
    "            self.coef_[0] = _theta\n",
    "            \n",
    "        return self.coef_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        if not self.no_bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        return np.where(self._sigmoid_function(X) < 0.5, 0, 1).ravel()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        if not self.no_bias:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        return self._sigmoid_function(X)\n",
    "    \n",
    "    def _logloss(self, y_pred, y):\n",
    "        pos = y.T @ np.log(y_pred)\n",
    "        neg = (1 - y).T @ np.log(1 - y_pred)\n",
    "        reg = self.coef_.T @ self.coef_\n",
    "        \n",
    "        return (- pos - neg + reg / self.C / 2) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイリスデータ\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# インスタンスを生成\n",
    "data = load_iris()\n",
    "\n",
    "# データをDataFrame型でXに格納する。\n",
    "X = pd.DataFrame(data.data, columns=['sepal_length',\n",
    "                                     'sepal_width',\n",
    "                                     'petal_length',\n",
    "                                     'petal_width'])\n",
    "# 目的変数も同様にyに格納する。\n",
    "y = pd.DataFrame(data.target, columns=['Species'])\n",
    "\n",
    "# X,yを結合\n",
    "df = pd.concat([X, y], axis=1) # axis=1 列方向に結合\n",
    "\n",
    "# 2値分類としたいため、以下の2つの目的変数のみ利用します。特徴量は4種類全て使います。virgicolorとvirginica\n",
    "df = df[df['Species']!=0]\n",
    "df['Species'] -= 1 # ({0: virgicolor, 1: virginica})\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 訓練データと検証データに分割\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df[['Species']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・スクラッチによる推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "slr = ScratchLogisticRegression(num_iter=1000, lr=0.1, C=5, no_bias=False, verbose=False)\n",
    "slr.fit(X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 推定\n",
    "slr_pred = slr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・sklearnによる推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 学習\n",
    "sgdc = SGDClassifier(loss='log', random_state=0)\n",
    "sgdc.fit(X_train_std, y_train)\n",
    "\n",
    "# 推定\n",
    "sgdc_pred = sgdc.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------混合行列--------------------\n",
      "scratch\n",
      "[[12  0]\n",
      " [ 0 13]]\n",
      "--------\n",
      "sklearn\n",
      "[[12  0]\n",
      " [ 1 12]]\n",
      "\n",
      "-----------predict--------------------\n",
      "scratch：[1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "sklearn：[1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "y_test ：[1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1]\n",
      "\n",
      "-----------評価値----------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>スクラッチ</th>\n",
       "      <th>sklearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F値</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           スクラッチ   sklearn\n",
       "Accuracy     1.0  0.960000\n",
       "Precision    1.0  1.000000\n",
       "Recall       1.0  0.923077\n",
       "F値           1.0  0.960000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scratch_eval = []\n",
    "sklearn_eval = []\n",
    "\n",
    "# Accuracy（正解率）\n",
    "from sklearn.metrics import accuracy_score\n",
    "scratch_eval.append(accuracy_score(y_test, slr_pred))\n",
    "sklearn_eval.append(accuracy_score(y_test, sgdc_pred))\n",
    "\n",
    "# Precision（適合率）\n",
    "from sklearn.metrics import precision_score\n",
    "scratch_eval.append(precision_score(y_test, slr_pred))\n",
    "sklearn_eval.append(precision_score(y_test, sgdc_pred))\n",
    "\n",
    "# Recall（再現率）\n",
    "from sklearn.metrics import recall_score\n",
    "scratch_eval.append(recall_score(y_test, slr_pred))\n",
    "sklearn_eval.append(recall_score(y_test, sgdc_pred))\n",
    "\n",
    "# F値\n",
    "from sklearn.metrics import f1_score\n",
    "scratch_eval.append(f1_score(y_test, slr_pred))\n",
    "sklearn_eval.append(f1_score(y_test, sgdc_pred))\n",
    "\n",
    "# 混合行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('-----------混合行列--------------------')\n",
    "print('scratch')\n",
    "print(confusion_matrix(y_test, slr_pred), )\n",
    "print('--------')\n",
    "print('sklearn')\n",
    "print(confusion_matrix(y_test, sgdc_pred))\n",
    "print('')\n",
    "print('-----------predict--------------------')\n",
    "print(f'scratch：{slr_pred}')\n",
    "print(f'sklearn：{sgdc_pred}')\n",
    "print(f'y_test ：{y_test.ravel()}')\n",
    "print('')\n",
    "print('-----------評価値----------------------')\n",
    "display(pd.DataFrame([scratch_eval, sklearn_eval], columns=('Accuracy', 'Precision', 'Recall', 'F値'), index=('スクラッチ', 'sklearn')).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('model loss')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(slr.loss, label=('train'), linestyle='--')\n",
    "plt.plot(slr.val_loss, label=('test'), linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】決定領域の可視化\n",
    "決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・sepal_lengthとpetal_lengthで描画する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイリスデータ\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# インスタンスを生成\n",
    "data = load_iris()\n",
    "\n",
    "# データをDataFrame型でXに格納する。\n",
    "X = pd.DataFrame(data.data, columns=['sepal_length',\n",
    "                                     'sepal_width',\n",
    "                                     'petal_length',\n",
    "                                     'petal_width'])\n",
    "# 目的変数も同様にyに格納する。\n",
    "y = pd.DataFrame(data.target, columns=['Species'])\n",
    "\n",
    "# X,yを結合\n",
    "df = pd.concat([X, y], axis=1) # axis=1 列方向に結合\n",
    "\n",
    "# 2値分類としたいため、以下の2つの目的変数のみ利用します。特徴量は4種類全て使います。virgicolorとvirginica\n",
    "df = df[df['Species']!=0]\n",
    "df['Species'] -= 1 # ({0: virgicolor, 1: virginica})\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 訓練データと検証データに分割\n",
    "X = df[['sepal_length', 'petal_length']].values\n",
    "y = df[['Species']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "slr = ScratchLogisticRegression(num_iter=1000, lr=0.1, C=5, no_bias=False, verbose=False)\n",
    "slr.fit(X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 推定\n",
    "slr_pred = slr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 学習\n",
    "sgdc = SGDClassifier(loss='log', random_state=0)\n",
    "sgdc.fit(X_train_std, y_train)\n",
    "\n",
    "# 推定\n",
    "sgdc_pred = sgdc.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定領域の描画関数\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "def decision_region(X, y, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor', 'virginica']):\n",
    "    \"\"\"\n",
    "    2値分類を2次元の特徴量で学習したモデルの決定領域を描く。\n",
    "    背景の色が学習したモデルによる推定値から描画される。\n",
    "    散布図の点は訓練データまたは検証データである。\n",
    "\n",
    "    Parameters\n",
    "    ----------------\n",
    "    X : ndarray, shape(n_samples, 2)\n",
    "        特徴量\n",
    "    y : ndarray, shape(n_samples,)\n",
    "        ラベル\n",
    "    model : object\n",
    "        学習したモデルのインスンタスを入れる\n",
    "    step : float, (default : 0.1)\n",
    "        推定値を計算する間隔を設定する\n",
    "    title : str\n",
    "        グラフのタイトルの文章を与える\n",
    "    xlabel, ylabel : str\n",
    "        軸ラベルの文章を与える\n",
    "    target_names= : list of str\n",
    "        凡例の一覧を与える\n",
    "    \"\"\"\n",
    "    # setting\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "    # pred\n",
    "    mesh_f0, mesh_f1  = np.meshgrid(np.arange(np.min(X[:,0])-0.5, np.max(X[:,0])+0.5, step), np.arange(np.min(X[:,1])-0.5, np.max(X[:,1])+0.5, step))\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "    # plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        plt.scatter(X[y==target][:, 0], X[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decision_region(X_test_std, y_test[:, 0], slr, step=0.01, title='Scratch of', xlabel='sepal_length', ylabel='petal_length', target_names=['versicolor', 'virginica'])\n",
    "decision_region(X_test_std, y_test[:, 0], sgdc, step=0.01, title='sklearn of', xlabel='sepal_length', ylabel='petal_length', target_names=['versicolor', 'virginica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】（アドバンス課題）重みの保存\n",
    "検証が容易になるように、学習した重みを保存および読み込みができるようにしましょう。pickleモジュールやNumPyのnp.savezを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = slr.coef_\n",
    "np.savez('np_savez.npz', w1)\n",
    "w2 = np.load('np_savez.npz')\n",
    "w2.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2[w2.files[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
