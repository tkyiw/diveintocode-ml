{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論理回路\n",
    "簡単な題材として、ロジスティック回帰による論理回路の再現を行います。論理回路は2つの値を入力し、1つの値を出力する関数のようなものです。入力も出力も0か1のみで表され、入力される組み合わせによって出力する値が変わります。\n",
    "\n",
    "\n",
    "ANDゲートは入力された2つの値が両方とも1だった場合、出力が1となり、それ以外の組み合わせでは0を出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの作成\n",
    "最初に学習用のトレーニングデータをNumPyにより作成しておきます。ANDゲートでは入力が2次元で出力が1次元となります。2つの入力が1のときだけ1を出力し、それ以外は0を出力するので以下のように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによる実装に必要な次の2つのことを順番に見ていきます。\n",
    "\n",
    "\n",
    "- データフローグラフを構築する\n",
    "- データを入力して計算する\n",
    "\n",
    "# データフローグラフの構築\n",
    "まずはデータフローグラフを構築します。\n",
    "\n",
    "\n",
    "学習データをTensorFlowのデータフローグラフに入力するための placeholder を用意しましょう。placeholderはデータフローグラフを作成する段階では値が決まっていない、空箱のような存在でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一引数のtf.float32で行列要素の数値のデータ型を指定しています。第二引数の[None,2]で行列の形を指定しています。ここで定義されている2はデータの次元を表しています。Noneの部分はデータ数を表す部分です。今回のANDゲートの場合のデータ数は[0,0],[0,1],[1,0],[1,1]の4つしかないのでNoneの部分を[4,2]としても問題はありません。しかし、任意の数のデータを入れられるように、一般的にはNoneを使います。\n",
    "\n",
    "\n",
    "tf.placeholder  |  TensorFlow\n",
    "\n",
    "\n",
    "重みとバイアスの Valiable を用意します。Valiableとして用意するということは、これらが学習により更新を行う値であることを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、tf.Variable()の中でtf.zeros()という関数を呼び出していますが、初期値として0を入れているということです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にモデルの出力y（＝仮定関数）と目的関数を定義します。\n",
    "\n",
    "\n",
    "ロジスティック回帰の式は以下でした。ここでは正則化項は抜かしています。\n",
    "\n",
    "\n",
    "$$h_θ(x) = g(θ^T x)$$\n",
    "$$g(z) = \\frac{1}{1+e^{−z}}$$\n",
    "$$J(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m}[−y^{(i)} log(h_θ(x^{(i)})) − (1−y^{(i)}) log(1−h_θ(x^{(i)}))]$$\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "\n",
    "$x$ : 特徴量ベクトル\n",
    "\n",
    "\n",
    "$\\theta$ : パラメータベクトル\n",
    "\n",
    "\n",
    "$g()$ : シグモイド関数\n",
    "\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解ラベル\n",
    "\n",
    "\n",
    "$\\theta_j$ : j番目のパラメータ（重み）\n",
    "\n",
    "\n",
    "$n$ : 特徴量の数\n",
    "\n",
    "\n",
    "これをTensorFlowで記述すると次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.matmul()はNumPyにおけるnp.dot()に相当するベクトルの内積や、行列積を計算するためのメソッドです。\n",
    "\n",
    "\n",
    "tf.math.sigmoid  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.log  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.reduce_sum  |  TensorFlow\n",
    "\n",
    "\n",
    "なお、例えば回帰問題で二乗和誤差関数を使用するのであれば、tf.reduce_sum(tf.square(y - t))というように定義できます。\n",
    "\n",
    "\n",
    "tf.math.square  |  TensorFlow\n",
    "\n",
    "\n",
    "ここまでで、入力のための空箱であるplaceholderと学習可能なValiableをメソッドで結ぶことができました。\n",
    "\n",
    "\n",
    "学習を行うために、勾配降下法を用いてパラメータを最適化するためのコードを加えます。目的関数をGradientDescentOptimizerに渡します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientDescentOptimizer()は引数で学習率を指定しています。\n",
    "\n",
    "\n",
    "tf.train.GradientDescentOptimizer  |  TensorFlow\n",
    "\n",
    "\n",
    "学習後の結果の正解が正しいかどうかの判定と正解率の計算もデータフローグラフとして定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1行目で結果が正解かどうか判定しています。一つ一つ見ていきましょう。まずtf.equal()は引数に指定された2つの値が等しいかどうかを判定してくれます。返り値はBool値です。tf.sign()は引数の値が正なら1、0なら0、負なら-1を返します。yが0.5以上かどうかで結果が決まるので、y-0.5とt-0.5の符号を比較しています。\n",
    "\n",
    "\n",
    "2行目は正解率を計算するためのコードです。tf.reduce_mean()は多次元配列の各成分の平均を計算する関数です。tf.cast()でBool値を0,1に変換しています。つまりここでは正解で1、不正解で0と判定された配列の平均値をとっているので正解率を表していることになります。\n",
    "\n",
    "tf.math.equal  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.sign  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.reduce_mean  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.dtypes.cast  |  TensorFlow\n",
    "\n",
    "\n",
    "以上で学習のための準備は終わりです。データフローグラフをコードで定義できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを入力して計算\n",
    "セッションを準備してパラメータを最適化する計算を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずセッションのインスタンスを作成します。そして、tf.global_variables_initializer()によって上で定義したtf.Variable()の値(重み・バイアス)を初期化します。実行する際にはsess.run()を使います。\n",
    "\n",
    "\n",
    "tf.initializers.global_variables  |  TensorFlow\n",
    "\n",
    "\n",
    "学習を行います。今回は1000回繰り返すことにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "epoch: 900, Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x:x_train,\n",
    "        t:y_train\n",
    "    })\n",
    "# 100回ごとに正解率を表示\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "               %(epoch, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2行目、sess.run()に上で定義したtrain_stepを入れることで、勾配降下法による学習を行っています。\n",
    "\n",
    "\n",
    "8行目はsess.run()にaccuracyを入れることで、正解率を計算しています。計算結果がNumPy形式で返ってきているので、これをprintします。\n",
    "\n",
    "\n",
    "形だけ定義していたplaceholderのxとtの中には値が何も入っていません。placeholderに値を設定するためにsess.run()のパラメータでfeed_dictを指定します。例えば、feed_dict={x:x_train,t:y_train})と書くことで空箱だったxにx_trainの値が入り、tにy_trainの値が入ります。\n",
    "\n",
    "\n",
    "表示結果を見てみると、正解率が100%でうまく学習できているように見えます。\n",
    "\n",
    "最後に各サンプルの計算結果を確認してロジスティック回帰の実装を終わります。先ほどのaccuracyと同様、実行することで計算結果が返ってくるためprintします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[1.9654632e-04]\n",
      " [4.9049824e-02]\n",
      " [4.9049824e-02]\n",
      " [9.3120384e-01]]\n"
     ]
    }
   ],
   "source": [
    "#学習結果が正しいか確認\n",
    "classified = sess.run(correct_prediction, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "#出力yの確認\n",
    "prob = sess.run(y, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "print(classified)\n",
    "# [[ True]\n",
    "# [ True]\n",
    "# [ True]\n",
    "# [ True]]\n",
    "print(prob)\n",
    "# [[  1.96514215e-04]\n",
    "# [  4.90498319e-02]\n",
    "# [  4.90498319e-02]\n",
    "# [  9.31203783e-01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiedの結果は全てTrueで正しく学習されていることがわかります。probは上からほぼ[0,0,0,1]となっています。今回活性化関数に用いたのはシグモイド関数ですので出力yは確率として表示されています。上から3つは1になる確率がほぼ0％、一番下の1つは93％程度の確率で1になるということになります。\n",
    "\n",
    "\n",
    "Wとbが学習後どのような値になっているかも見ておきましょう。Variableもsess.run()に入れることで値を確認できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.5699544]\n",
      " [5.5699544]]\n",
      "b: [-8.534579]\n"
     ]
    }
   ],
   "source": [
    "print('W:', sess.run(W))\n",
    "print('b:', sess.run(b))\n",
    "# W: [[ 5.5699544]\n",
    "# [ 5.5699544]]\n",
    "# b: [-8.53457928]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 途中の値が見たい場合\n",
    "デバッグのために途中の値が見たい場合もあります。例えば、y = tf.sigmoid(tf.matmul(x, W) + b)のtf.matmul(x, W)の部分が見たいといったことを考えます。\n",
    "\n",
    "\n",
    "出力yがsess.run(y, feed_dict={x:x_train, t:y_train})で見れたことと同様の行えば良いため、次のようなコードになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.       ]\n",
      " [ 5.5699544]\n",
      " [ 5.5699544]\n",
      " [11.139909 ]]\n"
     ]
    }
   ],
   "source": [
    "mat = tf.matmul(x, W)\n",
    "y = tf.sigmoid(mat + b)\n",
    "print(sess.run(mat, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "データフローグラフを構築し、それをsess.run()で実行するという流れはここでも同じです。\n",
    "\n",
    "\n",
    "# セッションの終了\n",
    "最後にセッションは終了させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with構文を使うことも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "mat = tf.matmul(x, W)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={x:x_train, t:y_train}) # ここに計算の実行コードを入れていく\n",
    "    ans = sess.run(mat, feed_dict={x:x_train, t:y_train})\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPyによるスクラッチ実装との比較\n",
    "TensorFlowを利用した場合、NumPyでのスクラッチのように更新の式ということを考える必要はなくなりました。データフローグラフの構築は、スクラッチにおけるフォワードプロパゲーションの実装に近いと言えます。また、シグモイド関数などよく使われるものは関数化されているため、それらを組み合わせていくだけで実装することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
