{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.このテキストについて\n",
    "\n",
    "テキストの目的\n",
    "Kerasの基本的な仕組みを知る\n",
    "\n",
    "どのように学ぶか\n",
    "サンプルコードとともに説明していきます。同じコードを打ち込んで実行していってください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Kerasとは\n",
    "\n",
    "Keras は元々はTheano、TensorFLowなど複数のフレームワークを扱いやすくする ラッパー として登場したライブラリでしたが、後にTensorFLowの 高レベルAPI として使われるようになりました。\n",
    "\n",
    "\n",
    "TensorFLowに含まれる形のKerasであるtf.kerasを主に使っていきます。\n",
    "\n",
    "\n",
    "《ラッパーとは》\n",
    "\n",
    "\n",
    "ラッパーはもともとのプログラムの機能を利用して、より使いやすいものを提供します。TensorFlowはニューラルネットワークに必要な計算を効率的に行う機能を提供しますが、初期のころはモデルを構築して学習を行うとなると手間がかかる部分もありました。そのため、TensorFlowをラップして、扱いやすくするKerasが登場しました。\n",
    "\n",
    "\n",
    "《高レベルAPIとは》\n",
    "\n",
    "\n",
    "大きな単位で機能を簡単に扱えるように作られたものが高レベルAPIです。対義語として、細かい単位で機能をいじれるが、扱いがその分大変な低レベルAPIがあります。\n",
    "\n",
    "\n",
    "TensorFlow自体でもニューラルネットワークのモデル構築や学習を行いやすくするために、高レベルAPIの充実が進められています。tf.Kerasはそのひとつです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.ロジスティック回帰の実装\n",
    "\n",
    "TensorFLow入門2と同様にロジスティック回帰によるANDゲートを作成してみます。\n",
    "\n",
    "\n",
    "はじめにANDゲートのデータを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# ANDゲートの学習データを用意\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数の記述方法\n",
    "Kerasでは簡素にニューラルネットワークが記述できます。その書き方にはSequentialモデルとFunctional APIの2種類があります。それぞれを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Sequentialモデル\n",
    "\n",
    "Sequentialクラスを使用した記述方法です。\n",
    "\n",
    "\n",
    "[tf.keras.models.Sequential | TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "\n",
    "\n",
    "層のインスタンスをSequentialクラスのコンストラクタにリストで渡すことでモデルを定義します。層のクラスについては以下のページにまとまっています。\n",
    "\n",
    "\n",
    "[Module: tf.keras.layers | TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "\n",
    "\n",
    "ロジスティック回帰を作るために、全結合層のクラス、tf.keras.layers.Denseを使います。引数に出力のユニット数、活性化関数、入力のユニット数を入れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, activation = tf.nn.sigmoid, input_shape=(2,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denseクラスは引数で重みの初期化方法、バイアスの有無などの指定も可能です。\n",
    "\n",
    "\n",
    "[tf.keras.layers.Dense | TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "\n",
    "\n",
    "作成したモデルの構造はsummaryメソッドで確認することができます。層ごとの出力のshapeとパラメータ数が併記されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "構造が記述できたら、モデルをコンパイルします。コンパイル時に損失関数と最適化手法、評価関数を指定します。損失関数は名前をstringで指定します。ここでは2値分類のため、binary_crossentropyとなります。多値分類の場合はcategorical_crossentropy、回帰の場合はmean_squared_errorのようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして学習を行います。scikit-learn同様にfitメソッドを使う設計になっています。verboseは学習過程の可視化方法のパラメータで、デフォルトの1ではバッチごとに更新されるプログレスバーが表示されます。verboseが0の場合は表示を行わず、2の場合はエポック毎の表示になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 263ms/sample - loss: 0.7021 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.6931 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 9ms/sample - loss: 0.6859 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 16ms/sample - loss: 0.6811 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.6715 - acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今は用意していませんが、検証用データがある場合は、引数validation_dataに与えることで、エポック毎の検証も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 4 samples\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 99ms/sample - loss: 0.6700 - acc: 0.7500 - val_loss: 0.6617 - val_acc: 0.7500\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 14ms/sample - loss: 0.6606 - acc: 0.7500 - val_loss: 0.6559 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 38ms/sample - loss: 0.6540 - acc: 0.7500 - val_loss: 0.6506 - val_acc: 0.7500\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 14ms/sample - loss: 0.6496 - acc: 0.7500 - val_loss: 0.6452 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 11ms/sample - loss: 0.6463 - acc: 0.7500 - val_loss: 0.6395 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推定もscikit-learn同様にpredictメソッドを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_proba [0.44367948 0.46207762 0.5131579  0.5316836 ]\n",
      "y_pred [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(x_train)[:, 0]\n",
    "# 確率を0, 1に変換\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果がいらず、評価のみ行う場合はevaluateメソッドも便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.639493465423584\n",
      "Train accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequentialモデルのもうひとつの書き方\n",
    "Sequentialモデルでは、コンストラクタで層のクラスを渡さず、addメソッドを使って記述する方法もよく使われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid, input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数層の場合\n",
    "ロジスティック回帰ではなく、2層のニューラルネットワークの場合は以下のように記述できます。2層目以降はinput_shapeを与える必要がありません。tf.kerasが自動的に計算するためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(2,)),\n",
    "            tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addメソッドを使えば次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(2,)))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Functional API\n",
    "\n",
    "Functional APIを使えばより自由度の高いモデル構築が行えます。Sequentialクラスの代わりにModelクラスを使用します。\n",
    "\n",
    "\n",
    "[tf.keras.models.Model | TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "\n",
    "\n",
    "入力から出力までの流れを記述していき、最後にModelクラスに入力層と出力層のインスタンスを渡します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(2,)) # 入力層\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(input_data) # 出力層\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル構造の記述以降はSequentialモデルと全く同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 226ms/sample - loss: 0.7071 - acc: 0.7500\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 10ms/sample - loss: 0.7000 - acc: 0.7500\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.6965 - acc: 0.7500\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.6927 - acc: 0.7500\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 5ms/sample - loss: 0.6903 - acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数層の場合\n",
    "4層のニューラルネットワークは以下のように記述できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この記述方法では枝分かれを表現することもできます。以下は3層目で2つに枝分かれし、次の層で結合している例です。\n",
    "\n",
    "\n",
    "[tf.keras.layers.concatenate | TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y1 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "z = tf.keras.layers.concatenate([y1, y2])\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(z)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.ラッパーとしてのKeras\n",
    "\n",
    "ラッパーとしてのKerasもデフォルトでTensorFlowをバックエンドとして使用しているため、基本的な使い方は同じです。\n",
    "\n",
    "\n",
    "ドキュメントが日本語でも公開されているため、tf.kerasを利用する上で参考にすることができます。例えば2つの記述方法については以下のページです。\n",
    "\n",
    "\n",
    "[Sequentialモデルのガイド - Keras Documentation](https://keras.io/ja/getting-started/sequential-model-guide/)\n",
    "\n",
    "\n",
    "[Functional APIのガイド - Keras Documentation](https://keras.io/ja/getting-started/functional-api-guide/)\n",
    "\n",
    "\n",
    "compileメソッドで指定できる損失関数もまとまっています。\n",
    "\n",
    "\n",
    "[損失関数 - Keras Documentation](https://keras.io/ja/losses/)\n",
    "\n",
    "\n",
    "Sequentialモデルは以下のように書けます。ロジスティック回帰の例です。\n",
    "\n",
    "\n",
    "以下のコードのほとんどは上で紹介したtf.kerasと実質的に同じですが、例えば活性化関数を全結合層とは別のクラスとして渡しています。また、最適化手法の部分はtf.train.AdamOptimizerからkeras.optimizers.Adamに変わっています。tf.kerasではTensorFlow自体の最適化手法クラスを呼んでいるのに対し、KerasではKeras独自の最適化手法クラスを使用するためです。ラッパーとしてのKerasのコードも見る機会が多いですから若干の違いに慣れておくと良いでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/iwahara/.pyenv/versions/anaconda3-2019.10/envs/deep/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.7976 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7883 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7838 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7818 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7766 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
